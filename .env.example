# LLM Configuration (Ollama only)
LLM_MODEL=llama3.2:3b                    # Model name in Ollama
OLLAMA_BASE_URL=http://localhost:11434/    # Ollama API endpoint

# Research Configuration
MAX_WEB_RESEARCH_LOOPS=3
SCRAPING_TIMEOUT_CONNECT=30
SCRAPING_TIMEOUT_READ=90

# Development Configuration
# Set to 'true' to use mock LLM client instead of connecting to Ollama
# Useful for development and testing without running Ollama server
DEBUG=true