# LLM Configuration (Ollama only)
LLM_MODEL=llama3.2:3b                    # Model name in Ollama
OLLAMA_BASE_URL=http://ollama:11434/     # Ollama API endpoint within Docker

# Research Configuration
MAX_WEB_RESEARCH_LOOPS=3
SCRAPING_TIMEOUT_CONNECT=30
SCRAPING_TIMEOUT_READ=90

# Development Configuration
# Set to 'true' to use mock LLM client instead of connecting to Ollama
# Useful for development and testing without running Ollama server
DEBUG=true

# API Configuration
API_HOST_PORT=8000                       # Host port for the API service
RESEARCH_API_PROJECT_NAME=ollama-deep-researcher  # Project name for Docker Compose volumes

